---
title: "A bit Advanced Prediction Task with mlr3"
author: "川田恵介"
format:
  revealjs:
    incremental: true
execute: 
  warning: false
  message: false
  eval: true
  echo: true
bibliography: references.bib
---

## Purpose

-   予測問題についての包括的な作業工程を例示

    -   ただしStackingは除く

-   TuningParameterの推定と本推定を同時に行うAutoTunerを活用

-   モデル比較を簡便かつ柔軟に行えるbenchmarkを活用

## RoadMap

```{mermaid}
flowchart LR
  B[もろもろ定義] --> C1[推定アルゴリズムの定義]
  B --> C2[+ パラメータTuning]
  C1 --> D[Trainデータを用いたベンチマーク]
  C2 --> D
  D --> E[最終推計]
  E --> F[Testデータによる評価]
```


## SetUp

```{r}
library(mlr3verse)
library(tidyverse)
lgr::get_logger("mlr3")$set_threshold("error") # Errorのみを表示
lgr::get_logger("bbotk")$set_threshold("error") # Errorのみを表示
future::plan("multisession") # 並列処理

set.seed(1)

Data <- read_csv("ExampleData/Example.csv")

Task <- as_task_regr(Data,
                     "Price") # Define Task

Subgroup <- partition(Task, ratio = 0.8)

R2 <- msr("regr.rsq") # Define Evaluation with R2

Mean <- lrn("regr.featureless") # Define SimpleMean
OLS <- lrn("regr.lm") # Define OLS
RandomForest <- lrn("regr.ranger") # Define Random Forest
```

## Tuning

-   TuningParameterの推定を行うアルゴリズムを定義

```{r}
CV <- rsmp("cv",folds = 2) # Define CrossValidation with 2 folds

Tuner <- tnr("random_search") # Define search method

Terminator <- trm("evals", n_evals = 100) # Define Terminal condition
```

## Pruned Tree

-   Prune Treeの推定

-   lts関数 (mlr3tuningpsaceパッケージ)が提供するおすすめ範囲内で探索 [@bischl]

```{r}
Tree <- lrn("regr.rpart") |> lts() # Define AdaptiveTree

Tree <- AutoTuner$new(
  learner = Tree,
  resampling = CV,
  measure = R2,
  tuner = Tuner,
  terminator = Terminator,
  store_models = TRUE
)

Tree$id <- "Tree"
```

## ElasticNet

-   ElasticNetを推定

-   lts関数 (mlr3tuningpsaceパッケージ)が提供するおすすめ範囲内で探索 [@bischl]

```{r}
LASSO <- lrn("regr.glmnet") |> lts() # Define LASSO

LASSO <- AutoTuner$new(
  learner = LASSO,
  resampling = CV,
  measure = R2,
  tuner = Tuner,
  terminator = Terminator,
  store_models = TRUE
)


LASSO$id <- "LASSO"
```

## BenchMaking

-   訓練データのみを用いて、アルゴリズムを比較 (TrainingData内のCrossValidation)

```{r}
Design <- benchmark_grid(
  tasks = Task$clone()$filter(Subgroup$train),
  learners = list(OLS,Tree,RandomForest,Tree,Mean,LASSO),
  resamplings = CV
)

Result <- benchmark(Design)

Result$aggregate(R2)
```

## Final Model

-   最善のアルゴリムであったRandomForest、及び全訓練データを用いて最終予測モデルを推定

```{r}
RandomForest$train(Task,Subgroup$train)

RandomForest$predict(Task,Subgroup$test)$score(R2) # Peformance in TestData
```

## Reference
