---
title: "部分線形モデル"
subtitle: "Semiparametric推定への応用"
author: "川田恵介"
format: 
  revealjs:
    html-math-method: katex
    css: styles.css
    slide-number: true
    incremental: true
self-contained: true
self-contained-math: true
bibliography: ref.bib
execute: 
  warning: false
  message: false
  eval: true
  echo: false
---

# Semiparametric推定への応用

```{r SetUp}
pacman::p_load(
  tidyverse,
  mlr3verse,
  mlr3pipelines,
  ranger,
  patchwork,
  DoubleML,
  fixest
)

source("SimTargetEstimationData.R")

lgr::get_logger("mlr3")$set_threshold("error")

lgr::get_logger("bbotk")$set_threshold("error")

OLS <- lrn("regr.lm")

Tree <- lrn("regr.rpart") |> lts()

Tree <- AutoTuner$new(
  learner = Tree,
  resampling = rsmp("cv",folds = 2),
  terminator = trm("evals", n_evals = 20),
  tuner = tnr("random_search")
  )

Stacking <- pipeline_stacking(
  base_learners = list(
    OLS,
    Tree),
  super_learner = lrn(
    "regr.lm",
    id = "SL"),
  folds = 2,
  use_features = FALSE
  ) |> 
  as_learner()

Data <- SimData(1,100)

SimpleTree <- function(Formula,Data){
  Fit <- rpart::rpart(
    as.formula(Formula),
    Data
  )
  CP <- Fit$cptable[which.min(Fit$cptable[, "xerror"]), "CP"]
  PruneFit <- rpart::prune(Fit,cp = CP)
  return(PruneFit)
}

```

- 教師付き学習を、母集団の記述統計量推定に応用

- Semiparametric推定の文脈に落とし込む

    - 教師付き学習 $=$ 多次元でも実用的な(擬似)Nonparametric推定法

## 動機

- 経済学におけるデータ分析の主要な関心は、母集団の重要な特徴(因果効果,格差など)の理解

- 教師付き学習 $:=$ 母平均関数 $E_P[Y|X]$ の近似関数 $g(X)$ の推定

    - $g(X)$ を母平均関数理解に使えるか？

    - 複雑な $g(X)$ の特徴を理解する手法は多く存在 [@molnar2022interpretable]
    
- 問題点: Well-specified modelを"OLS"推定した場合と比べて、推定誤差の定量化が難しい

## 例 Stacking (OLS with 2次項 + 剪定ずみ決定木)

```{r}
N <- 100000

Data <- tibble(
  X = runif(N,-2,2),
  D = sample(0:1,N,replace = TRUE),
  U = rnorm(N,0,10)
  ) |> 
  mutate(
    Y = X^2 + D + if_else(X >= 1,1,0) + if_else(X >= 0, 1,0) + if_else(X >= -1,1,0) + U
  )

Mutate <- po("mutate")
Mutate$param_set$values$mutation = list(
  X2 = ~ X*X
)

OLS <- Mutate %>>% lrn("regr.lm") |> as_learner()

Tree <- lrn("regr.rpart") |> lts()

Tree <- AutoTuner$new(
  learner = Tree,
  resampling = rsmp("cv",folds = 2),
  terminator = trm("evals", n_evals = 20),
  tuner = tnr("random_search")
)

Stacking <- pipeline_stacking(
  base_learners = list(
    OLS,
    Tree),
  super_learner = lrn(
    "regr.lm",
    id = "SL"),
  folds = 2,
  use_features = FALSE
) |> 
  as_learner()

Task <- as_task_regr(
  Data |> select(Y,D,X),
  "Y"
)

Stacking$train(Task)


Fig <- Data |> 
  mutate(
    D = D |> factor(),
    Pred = Stacking$predict(Task)$response,
    TrueY = Y - U) |> 
  ggplot(
    aes(
      x = X,
      y = TrueY,
      color = D
    )
  ) +
  theme_bw() +
  geom_line(
    aes(linetype = "Population")
  ) +
  geom_line(
    aes(
      y = Pred,
      linetype = "Stacking"
    )
  ) +
  ylab("E[Y|D,X]")

Fig
```

- 10万サンプルで推定しても、ズレている

## 主要参考文献

- (Double/)Debiased Machine Learning [@chernozhukov2018double]

    - 関連ワード: Neyman's ohtogonality/Locally robust score/Efficient influence function [@chernozhukov2022locally], Mixed bias property [@rotnitzky2021characterization]

- 大量の解説論文 [@ichimura2022influence; @fisher2021visually; @Hines2021DemystifyingSL]

- 教師付き学習の有力な応用 [@leist2022mapping]

## 実装

- [DoubleML (R/Python)](https://github.com/DoubleML/doubleml-for-py)

- [grf (R)](https://github.com/grf-labs/grf)

- [tlverse (R)](https://github.com/tlverse)

- [econml (Python)](https://pypi.org/project/econml/)

    - [STATA](https://www.stata.com/manuals/lassoporegress.pdf)

- [日本語での紹介](https://github.com/tetokawata/BookEmiricalSocialML)

# Quick Start

- $X$ を一定とした下 (Control) で、 $D$ と $Y$ の関係性を推定する

## 部分線形モデル

- Partial Linear Model

$$Y_i=\underbrace{\tau_P}_{Estimand (関心となる特徴)}\times D_i+\underbrace{b(X_i)}_{未知} + \underbrace{u_i}_{E[u|D,X]=0}$$

- 主要な仮定: $\tau_P$ は、 $X,D$ に"依存していない"

    - Misspecificationが生じていたとしても、母分布上で解釈可能 (@vansteelandt2022assumption)

## Partialling-out algorithm

1. $Y,D$ の予測モデル $g_Y(X),g_D(X)$ を、何らかの方法で交差推定する

2. 予測誤差 $Y-g_Y(X),D-g_d(X)$ を単回帰する ($Y-g_Y(X)\sim D-g_D(X)$)

3. $\tau_P$ の推定値 $=$ 単回帰の係数

## 主要な性質

- "緩やかな"仮定のもとで、 一致/漸近正規性を満たす

    - **C**onsistent and **A**symptotically **N**ormal (CAN) estimator

    - 信頼区間の近似計算が可能

## 他の手法の問題点

- 近似モデル $g(D,X) \simeq E[Y|D,X]$ を推定し、 $E[g(1,X)-g(0,X)]$ を計算する
    
    - Plugin-in estimator
    
    - 一般にCAN estimatorにならない
    
- OLSで推定: 深刻な定式化依存 $\rightarrow$ Not consistent

- 教師付き学習: 収束が遅い $\rightarrow$ (May be) consistent but not Asymptotically normal

## 数値例

- 「格闘ゲームをプレイした経験間で、主観的幸福度はどの程度異なるのか？」

    - 年齢と主観的幸福度、格闘ゲームのプレイ経験には強い相関がされるので、"コントロール"
    
- 母集団

    - 格闘ゲームのプレイ経験があるグループの方が、主観的幸福度は高い
    
    - 40歳前後が最も格闘ゲームのプレイ経験は高い
    
    - 年齢と主観的幸福度の間には、U字の関係がある

## 数値例

```{r}
SimData(1,200) |> 
  mutate(
    D = factor(D)
  ) |> 
  ggplot(
    aes(
      x = X,
      y = Y,
      color = D
    )
  ) +
  theme_bw() +
  geom_point()
```

- $Y\sim\beta_0+\beta_1D + \beta_2 X$ を行うと?

## 数値例

```{r}
SimData(1,200) |> 
  mutate(
    D = factor(D),
    TrueY = Y - U
  ) |> 
  mutate(
    Pred = lm(Y ~ D + X, SimData(1,200)) |> predict(),
    PredCorrect = lm(Y ~ D + I(X^2), SimData(1,200)) |> predict()
  ) |> 
  ggplot(
    aes(
      x = X,
      y = Y,
      color = D,
      group = D
    )
  ) +
  theme_bw() +
  geom_point(
    alpha = 0.5
  ) +
  geom_line(
    aes(
      y = PredCorrect,
      linetype = "Population"
    )
  ) +
  geom_line(
    aes(
      y = Pred,
      linetype = "OLS"
    )
  )
```

## まとめ

- Partialling-out自体は古典的なアイディア (少なくとも @Robinson1988ROOTNCONSISTENTSR )

    - 理論性質についても、精緻な議論がされていた

- 教師付き学習 $+$ 交差推定を組み込むことで、より幅広い状況に応用が可能に

- 他の応用 (例: AIPW, Sensitivity, Panel Data, Mediation Analysis) と、同様の原理を共有する

# Reserch RoadMap

- データ分析をめぐる手法や概念について、依然として混乱が見られる

    - 過度に"万能"視

    - 過剰な縦割り/縄張り意識

- "研究RoadMap"の中にしっかり埋め込んで理解/整理される必要がある

## 実証分析のRoadMap

1. Research Question: 知りたい(母)分布の特徴は?

2. Identification Step: 観察できる変数のみで書き下せるか?

3. Summary Step: 推定 & 理解可能な程度に単純化

4. Estimation Step: 推定 & 理解可能な程度に単純化

5. Coding & 分析 Step


## 大雑把な整理

1. Research Question: 実務/研究(理論的)動機: 因果推論、比較研究、予測研究

2. Identification Step: 潜在結果/RegressionDiscontinuity/IV/DAG

3. Summary Step: 線形モデル, 周辺化条件付き平均差, 分位点差

4. Estimation Step: 教師学習/セミパラ推定/最尤法・ベイズによるパラメトリック推定

5. Coding & 分析 Step

- 異なるQuestionについて、同じ推定手法は使える場合も多い

## 例: Research Question

- 賃金($=Y$)、大卒/高卒($=X$)、Windows/LinuxOS User($=OS$) が使えるとして、

1. 同一教育年数内OS間賃金格差 (比較/格差研究)

2. 同一教育経験内OS間賃金格差 (比較/格差研究)

3. OSが賃金に与える因果効果 (因果効果)

## 例: Identification

1. 同一教育年数内賃金格差 (比較/格差研究): 以下を比較すればいいので"不要"

$$f_P(Y|Linux, X)\ VS\ f_P(Y|Windows,X)$$

2. 同一教育経験内賃金格差 (比較/格差研究): 教育経験が観察できないので、たとえば、以下の仮定が必要

$$f_P(Y|OS, 教育経験) = f_P(Y|OS, 教育年数)$$

## 例: Causal Identification

- 因果効果の定義とIdentificationについては、膨大な議論が存在 [@imbens2022causality など]

- 典型的な仮定は、 Conditional unconfounderness/independence

$$f_P(Y|OS, X) = f_P(Y|OS,X,U)$$

- $U$ : OS選択の"前に"決まる全ての観察できない変数

    - 例えば、使用するOSがランダムに強制決定されていればOK

## 例: Summary

- 一般に条件付き分布 $f_P(Y|OS,X)$ の $OS$ についての比較は難しい

- 解釈が容易で推定可能な母集団上での記述統計量 (Estimand) を定義する

- 典型的なEstimandは、

    - $\tau_P(X)=E_P[Y|Linux,X]-E_P[Y|Windows,X]$
    
    - $\tau_P=E_P[\tau_P(X)]$

- Research Question 1-3 まで全てに"有効"

## 例: Estimation

- Estimandを有限サンプルから推定する

- OLS, 最尤法, ベイズ, 傾向スコア, 教師付き学習などなど

    - ここではセミパラ推定 $+$ 教師付き学習

## まとめ

- Identificationが大きく異なったとしても、同じSummaryやEstimationが活用可能なケースは多い

- 現状、教師付き学習の最も確立された応用先は、Estimation

    - 最後に他のStepへの応用可能性も紹介

# Summary

- Misspecificationが生じた部分線形モデルは何を推定している?

    - Estimandは?

## 条件付き平均差

```{r}
tibble(
  X = c("高校卒","大学卒"),
  `Tau_P(X)` = c(20,10),
  `E_P[Linux|X]` = c(0.6,0.001),
  `f_P(X)` = c(0.6,0.4)
)
```

- サブサンプル平均差で推定できる場合もある

    - $X$ の値が増えると、サブサンプルサイズが非常に小さくなり、不可能になる
    
## 周辺化

- より推定が容易な目標

$$\tau_P=\underbrace{W(高校)}_{Weight}\times \underbrace{\tau_P(高校)}_{=20} +\underbrace{W(大学)}_{Weight}\times \underbrace{\tau_P(大学)}_{=10}$$

- Weightは、"本質的には"、任意

    - $W(大学)=W(高校)=0.5$ ならば、 $\tau=15$

    - $W(大学) = 0.4 (大卒比率),W(高校) = 0.6 (高卒比率)$　ならば、 $\tau=16$

## Variance-weights

- 周辺化された条件付き平均差: ただし

$$W(x)=\frac{V_P(OS|x)\times f_P(x)}{V_P(OS)}$$

- $V_P(OS|X)=E_P[(OS-E_P[OS|E])^2|X]$ (OSの分散)

    - $W(大学)\simeq 0,W(高校)\simeq 1$
    
    - $\tau_P\simeq 20$
    
- あまり直感的ではないかも (代替案: AIPW)
  
## 詳細: 単回帰の復習

- OLSの係数値 $=$ 共分散/分散

- 同じ$X$を共有するサブグループ(母集団)内で、$Y=\beta_0+\beta_1 D+u$ を回帰すると

$$\tau_P(X):=\beta_1=\frac{cov_P(Y,D|X)}{var_P(D|X)}$$

$$=\frac{E_P[(Y-E_P[Y|X])(D-E_P[D|X])|X]}{E_P[(D-E_P[D|X])^2|X]}$$

## 詳細: Partialling-outの推定結果

- $Y-E_P[Y|X]\sim D - E_P[D|X]$ を母集団で回帰すると、

$$\tau_P=\frac{E_P[(Y-E_P[Y|X])(D-E_P[D|X])]}{E_P[(D-E_P[D|X])^2]}$$

- 繰り返し期待値 $E[Y]=\int E[Y|X]\times f(X)dX$ を使うと?

## 詳細: Partialling-outの推定結果

$$\tau=\int \frac{E_P[(Y-E_P[Y|X])(D-E_P[D|X])|X]}{E_P[(D-E[D|X])^2]}f_P(X)dX$$

$$=\int \underbrace{\frac{E_P[(D-E_P[D|X])^2|X]}{E_P[(D-E_P[D|X])^2]}\times f_P(X)}_{=W(X)}$$

$$\times\underbrace{\frac{E[(Y-E[Y|X])(D-E[D|X])|X]}{E[(D-E[D|X])^2|X]}}_{=\tau(X)}dX$$

## まとめ

- Partialling-out推定は、 母集団における記述統計"単回帰の加重平均" ($Y$ と $D$ のBLP) について、信頼区間を提供

    - $D$ が二値であれば、周辺化された条件付き平均差

- OLSは母集団におけるBest Linear Projectionについて、信頼区間を提供

## 例外

- 一般にBLPと周辺化条件付き平均差は一致しないが、重要な例外も存在

    - BLP $= E_P[Y|D,X]$、 または、

- $D$ と $X$ が独立 ($D$ がランダムに決定されているなど)であれば、 BLPにおける$D$ の係数値 $=$ 周辺化された条件付き平均差
    
    - 理想的な自然実験に近いデータについて、ダメ押しとしてOLSを使うのであれば、大きな問題はない
    
    - (かつて?) TopJournalに掲載された実証研究に多いので、それを手本とした研究も引っ張れがち

## Reference

