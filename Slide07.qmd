---
title: "Estimate structural parameter/function"
subtitle: "機械学習の経済学への応用"
author: "川田恵介"
format:
  revealjs:
    incremental: true
    self-contained-math: true
    slide-number: true
execute: 
  message: false
  warning: false
  echo: false
  eval: true
bibliography: references.bib
---

```{r}
pacman::p_load(DiagrammeR,
               tidytable,
               recipes,
               mlr3verse,
               mlr3pipelines,
               mlr3tuning,
               mlr3extralearners,
               DoubleML,
               estimatr,
               ggplot2,
               stringr,
               grf,
               tidytable,
               patchwork)

SimData <- SimData <- function(i,n,extrime){
  set.seed(i)
  X <- runif(n,0,1)
  Y <- 2*if_else(X >= 0.9,1,0) + rnorm(n,0,10)
  TempData <- tidytable(X = X, Y = Y) |> 
    bind_rows.(tidytable(X = 1,
                     Y = 2 + qnorm(extrime,0,10)))
  return(TempData)
}

Data <- fread("ExampleData/Example.csv")

Group <- sample(0:1,nrow(Data),replace = TRUE)

Y <- Data$Price
D <- Data$Reform
X <- recipe(~ TradeQ + Size + BuildYear + Distance,
            Data) |> 
  step_normalize(all_predictors()) |> 
  prep() |> 
  bake(new_data = NULL,
       composition = "matrix")

RegOLS <- po("colapply",  
             applicator = function(x){poly(x,row = TRUE, degree = 2)}) %>>%
  lrn("regr.lm",
      id = "OLS") |> 
  as_learner()

ProbOLS <- po("colapply",  
              applicator = function(x){poly(x,row = TRUE, degree = 2)}) %>>%
  lrn("classif.log_reg",
      id = "Logit") |> 
  as_learner()

RegBART <- lrn("regr.bart",
               id = "BART")
RegBART$param_set$values$verbose <- FALSE

ProbBART <- lrn("classif.bart",
               id = "BART")
ProbBART$param_set$values$verbose <- FALSE


RegLearner <- pipeline_stacking(base_learners = list(RegOLS,
                                                     RegBART,
                                                     lrn("regr.ranger",
                                                         id = "RF")
                                                     ),
                                super_learner = lrn("regr.lm",
                                                    id = "Aggregate"),
                                folds = 2,
                                use_features = FALSE
                                ) |> 
  as_learner()

ProbLearner <- pipeline_stacking(base_learners = list(ProbOLS,
                                                      ProbBART,
                                                      lrn("classif.ranger",
                                                          id = "RF")),
                                super_learner = lrn("classif.log_reg",
                                                    id = "Aggregate"),
                                folds = 2,
                                use_features = FALSE
                                ) |> 
  as_learner()
```

## Goal

-   **母集団における**条件付き平均差: $\tau(d',d,X)=E[Y|D=d',X]-E[Y|D=d,X]$ の**特徴を推論**

    -   漸近正規性を根拠とした信頼区間推定

    -   機械学習を活用した統計モデル依存の緩和

    -   セミパラメトリック推定を活用した収束性質の改善

## MainGoal

1.  $\tau(d',d,X)\simeq \underbrace{\beta_0+\beta_1X_1,..,\beta_LX_L}_{記述モデル}$ の推論

2.  $\tau(d',d,X)$ のNonparametric推定 (Tree系統)

3.  $\tau(d',d,X,\underbrace{U}_{Unobservable})\simeq \beta_0$ の推論

-   クロスセクション、および2時点パネル(かつ $D$ がカテゴリ変数)を想定

## とりあえずのゴール

-   Partial Linear Model [@robinson1988] $E[Y|D,X]=\tau\times D+g(X)$ の推定

-   Partialling-out推定

1.  $Y,D$の予測モデル$f_Y(X),f_D(X)$を推定（機械学習も可）

2.  予測できなかった部分 $Y-f_Y(X),D-f_D(X)$ を抽出し(partialling-out)、 OLSで回帰

3.  Robust Standard Errorを計算し、(漸近)信頼区間を形成

## Gallary: MarginalMeanDifference

-   @chernozhukov2018, @chernozhukov2022

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bblock")$set_threshold("warn")
Q <- qnorm(1-(0.05/2))

Task <- double_ml_data_from_matrix(
  X = X[Group == 0,],
  d = D[Group == 0],
  y = Y[Group == 0]
)

TaskConfirm <- double_ml_data_from_matrix(
  X = X[Group == 1,],
  d = D[Group == 1],
  y = Y[Group == 1]
)


PLR <- DoubleMLPLR$new(Task,
                       RegLearner$clone(),
                       RegLearner$clone(),
                       n_folds = 2)$
  fit(store_predictions = TRUE)

OLS <- DoubleMLPLR$new(Task,
                       lrn("regr.lm"),
                       lrn("regr.lm"),
                       n_folds = 1,
                       apply_cross_fitting = FALSE
                       )$
  fit(store_predictions = TRUE)

tibble(Est = c(PLR$all_coef,
               OLS$all_coef),
       SE = c(PLR$all_se,
              OLS$all_se),
       Method = c("Partialling-out with ML",
                  "OLS")
       ) |> 
  ggplot(aes(y = Method,
             x = Est,
             xmin = Est - Q*SE,
             xmax = Est + Q*SE)
        ) +
  geom_pointrange() +
  theme_bw() +
  geom_vline(xintercept = 0) +
  xlab("E[tau(X)]")
```

## Gallary: BLP

-   @semenova2021

```{r}
Q <- qnorm(1-(0.05/(2*4)))

AIPW <- DoubleMLIRM$new(Task,
                       RegLearner$clone(),
                       ProbLearner$clone(),
                       n_folds = 2
                       )$
  fit(store_predictions = TRUE)

AIPW_Confirm <- DoubleMLIRM$new(TaskConfirm,
                       RegLearner$clone(),
                       ProbLearner$clone(),
                       n_folds = 2
                       )$
  fit(store_predictions = TRUE)

AIPW_OLS <- DoubleMLIRM$new(Task,
                       lrn("regr.lm"),
                       lrn("classif.log_reg"),
                       n_folds = 2
                       )$
  fit(store_predictions = TRUE)

BLP <- lm_robust(AIPW$psi_b ~ X[Group == 0,],
                 alpha = 0.05/4) |> 
  tidy() |> 
  mutate.(Method = "AIPW with ML")

BLP_OLS <- lm_robust(AIPW_OLS$psi_b ~ X[Group == 0,],
                 alpha = 0.05/4) |> 
  tidy() |> 
  mutate.(Method = "AIPW with OLS&Logit")


BLP |> 
  bind_rows.(BLP_OLS) |> 
  ggplot(aes(y = term,
             x = estimate,
             xmin = conf.low,
             xmax = conf.high,
             color = Method)
        ) +
  geom_pointrange() +
  theme_bw() +
  geom_vline(xintercept = 0) +
  xlab("E[tau(X)]")
```

## Gallary: CausalTree

-   @athey2016

```{r}
Trim <- AIPW$predictions$ml_m

GroupTree <- partykit::ctree(Y ~ .,
                data.table(X) |> 
                  filter.(Group == 0) |> 
                  mutate.(Y = AIPW$psi_b[,1,1]),
                control = partykit::ctree_control(alpha = 0.01,
                                                  minsplit = 1000,
                                                  minbucket = 500),
                subset = Trim >= 0.1 & Trim <= 0.9
                )

plot(GroupTree,
     type = "simple")
```

## Gallary: ConfirmTree

-   @athey2016

```{r}
TempGroup <- GroupTree |> 
  predict(data.table(X) |>
            filter.(Group == 1),
          type = "node") |> 
  factor()

lm_robust(Y ~ 0 + G,
          data.table(Y = AIPW_Confirm$psi_b[,1,1],
                 G = TempGroup),
          subset = AIPW_Confirm$predictions$ml_m >= 0.1 & 
            AIPW_Confirm$predictions$ml_m <= 0.9) |> 
  tidy() |> 
  ggplot(aes(y = term,
             x = estimate,
             xmin = conf.low,
             xmax = conf.high)
        ) +
  geom_pointrange() +
  geom_vline(xintercept = 0) +
  theme_bw()
```

## Gallary: EvilTree

-   @athey2016

```{r}
TempGroup <- GroupTree |> 
  predict(data.table(X) |>
             filter(Group == 0),
          type = "node") |> 
  factor()

lm_robust(Y ~ 0 + G,
          data.table(Y = AIPW$psi_b[,1,1],
                 G = TempGroup[Group == 0]),
          subset = Trim >= 0.1 & Trim <= 0.9) |> 
  tidy() |> 
  ggplot(aes(y = term,
             x = estimate,
             xmin = conf.low,
             xmax = conf.high)
        ) +
  geom_pointrange() +
  geom_vline(xintercept = 0) +
  theme_bw()
```

## Gallary: RandomForest

-   @athey2019, @wager2018

```{r}
CF <- causal_forest(X = X[Group == 0,],
                    W = D[Group == 0],
                    Y = Y[Group == 0],
                    Y.hat = PLR$predictions$ml_l[,1,1],
                    W.hat = PLR$predictions$ml_m[,1,1]
                    )

predict(CF, estimate.variance = TRUE) |> 
  arrange.(predictions) |> 
  mutate.(ID = c(1:length(Y[Group == 0]))) |> 
  ggplot(aes(x = ID,
             y = predictions,
             ymin = predictions - 1.96*sqrt(variance.estimates),
             ymax = predictions + 1.96*sqrt(variance.estimates))
         ) +
  geom_line() +
  geom_ribbon(alpha = 0.5) +
  theme_bw()
```

## Gallary: RATE

-   @yadlowsky2021

```{r}
rank_average_treatment_effect(CF,
                              CF$predictions) |> 
  plot()
```

## Gallary: SortedCATE

-   @kallus2022

```{r}
EstScore <- RegLearner$
  clone()$
  train(as_task_regr(data.table(X) |> 
                       filter.(Group == 0) |> 
                       mutate.(Y = AIPW$psi_b[,1,1]),
                     "Y",
                     id = "Estimation"
                     ),
        )$
  predict(as_task_regr(data.table(X) |> 
                       filter.(Group == 1) |> 
                       mutate.(Y = AIPW_Confirm$psi_b[,1,1]),
                     "Y",
                     id = "Confirmation"
                     )
          )

TempFunction <- function(q) {
  Q <- quantile(EstScore$response, q)
  Score <- Q +
    (if_else(EstScore$response >= Q, 1, 0) / (1 - q)) *
      (AIPW_Confirm$psi_b[, 1, 1] - Q)
  TempResult <- lm_robust(Score ~ 1) |>
    tidy() |>
    mutate.(term = q)
  return(TempResult)
}

map_dfr.(seq(0,0.9,0.1),
         TempFunction) |> 
  ggplot(aes(x = term,
             y = estimate,
             ymin = conf.low,
             ymax = conf.high)
         ) +
  geom_line() +
  geom_ribbon(alpha = 0.5) +
  theme_bw()
```

## Gallary: Sensitivity

-   @chernozhukov, @cinelli2020

```{r}
TaskShort <- double_ml_data_from_data_frame(Data |> 
                                              filter.(Group == 0) |> 
                                              as.data.table(),
  x_cols = c("TradeQ", "Size" ,"BuildYear"),
  y_col = c("Price"),
  d_cols = c("Reform")
)

AIPWShort <- DoubleMLIRM$
  new(TaskShort,
      RegLearner$clone(),
      ProbLearner$clone(),
      n_folds = 2
      )$
  fit(store_predictions = TRUE)

Alpha <- if_else(Data$Reform[Group == 0] == 1,1,0)/AIPW$predictions$ml_m -
  (if_else(Data$Reform[Group == 0] == 0,1,0)/(1-AIPW$predictions$ml_m))

AlphaShort <- if_else(Data$Reform[Group == 0] == 1,1,0)/AIPWShort$predictions$ml_m -
  (if_else(Data$Reform[Group == 0] == 0,1,0)/(1-AIPWShort$predictions$ml_m))

Tau <- AIPW$all_coef[1,1]
Sigma2 <- mean((Data$Price[Group == 0] - if_else(Data$Reform[Group == 0] == 1,
                                    AIPW$predictions$ml_g1,
                                    AIPW$predictions$ml_g0))^2)
V2 <- mean(Alpha^2)

ScoreTau <- Tau - AIPW$psi_b
ScoreSigma2 <- Sigma2 - (Data$Price[Group == 0] - if_else(Data$Reform[Group == 0] == 1,
                                    AIPW$predictions$ml_g1,
                                    AIPW$predictions$ml_g0))^2
ScoreV2 <- V2 - 2*(AIPW$predictions$ml_g1 - AIPW$predictions$ml_g0) + Alpha^2

RegAlpha <- lm(Alpha ~ AlphaShort) |> 
  summary()

RegY <- lm(Y ~ D,
           data.table(Y = Data$Price[Group == 0] - 
                        if_else(Data$Reform[Group == 0] == 1, 
                                AIPWShort$predictions$ml_g1,
                                AIPWShort$predictions$ml_g0),
                      D = if_else(Data$Reform[Group == 0] == 1, 
                                AIPW$predictions$ml_g1,
                                AIPW$predictions$ml_g0) -
                        if_else(Data$Reform[Group == 0] == 1, 
                                AIPWShort$predictions$ml_g1,
                                AIPWShort$predictions$ml_g0))
           ) |> 
  summary()

CD <- sqrt((1-RegAlpha$r.squared)/RegAlpha$r.squared)

CY <- sqrt(RegY$r.squared)

S <- sqrt(mean((Data$Reform[Group == 0] - if_else(Data$Reform[Group == 0] == 1, 
                       AIPWShort$predictions$ml_g1,
                       AIPWShort$predictions$ml_g0))^2)*
  mean(AlphaShort^2))

B <- S*CY*CD

c(Tau - B, Tau + B)

ScoreLower <- ScoreTau - 0.5*((CY*CD)/S)*(Sigma2*ScoreV2 + V2*ScoreSigma2)
ScoreUpper <- ScoreTau - 0.5*((CY*CD)/S)*(Sigma2*ScoreV2 + V2*ScoreSigma2)

c(Tau - B - qnorm(1-(0.05/2))*sqrt(mean(ScoreLower^2)/nrow(Data)), 
  Tau + B + qnorm(1-(0.05/2))*sqrt(mean(ScoreUpper^2)/nrow(Data))
  )

data.table(
  Type = c("Point", 
           "Bound", 
           "PointCI", 
           "BoundCI"),
  Point = rep(Tau,4),
  Lower = c(Tau, 
            Tau - B, 
            Tau - qnorm(1 - (0.05 / 2)) * AIPW$all_se, 
            Tau - B - qnorm(1 - (0.05 / 2)) * sqrt(mean(ScoreLower^2) / nrow(Data))),
  Upper = c(Tau, 
            Tau + B, 
            Tau + qnorm(1 - (0.05 / 2)) * AIPW$all_se, 
            Tau + B + qnorm(1 - (0.05 / 2)) * sqrt(mean(ScoreUpper^2) / nrow(Data)))
  ) |>
  ggplot(aes(y = Type,
             x = Point,
             xmin = Lower,
             xmax = Upper)) +
  geom_pointrange() +
  geom_vline(xintercept = 0) +
  theme_bw()
```


## 基本方針

-   教師付き学習の目標: $f(X)\in\arg\min E[(E[Y|X]-\underbrace{f(X)}_{BlackBox})^2]$

    -   OLS: $f(X)=\beta_0+\beta_1X_1,..,\beta_LX_L$ の不偏推定

-   母分布全体の正確な推定は不可能

-   "推論"も困難

    -   伝統的推定: 統計モデル定式化への依存

    -   機械学習: 漸近性質の悪化

-   分布の**部分的な特徴**のみを推論するのであれば？

    -   社会構造（格差、因果効果など）研究では"十分"

## なぜ十分か？

-   "特定の"解釈が可能な母集団の特徴を推定

    -   データの外で決定

-   例: $E[Skill|OS=Linux,Age]-E[Skill|OS=Others,Age]$

    -   使用OS間不平等として解釈可能？

    -   使用OSの因果効果として解釈可能？

-   Age は**関心外** (Nuisance変数)

## なぜ困難か？

-   母平均を完璧に推定することは非現実的

    -   正しいモデルをどれだけ大きなデータ($\neq$ 無限大)で推定したとしても、一般に正しい値と一致しない

## Repeated Sampling Approach

-   諸々の仮定が"正しければ"、"信頼区間"は形成できる

    -   同じ手法で独立したサンプリングを行う大量の"並行世界"を想定

    -   多くの世界(95%など)で、真の値を含んだ区間を得られる

-   Moving the GoalPost

## 数値例: 収束

$$E[Y|X]=2\times I(X>=0.9)$$

-   $Y-E[Y|X]\sim N(0,10)$

-   シナリオ: 最初のサンプルで上振れ($X=1$ の上位99%)

-   アルゴリズム: CorrectTree ($I(X\ge 0.9)$ を $Y$ に回帰), WrongTree ($I(X\ge 0.5)$ を $Y$ に回帰), AdaptiveTree (最大2分割, 最小 サンプルサイズ = 5)

## 数値例: 収束

```{r}
i <- 2
e <- 0.99

Result <- map_dfr(1:1000, function(n) {
  TempModel <- rpart::rpart(Y ~ X,
    SimData(i, n, e),
    control = rpart::rpart.control(
      cp = 0,
      maxdepth = 1,
      minsplit = 4,
      minbucket = 2
    )
  )
  TempAdaptiveTree <-  TempModel |>
    predict(tibble(X = 1))
  
  TempCorrectTree <- lm(
    Y ~ I(X >= 0.9),
    SimData(i, n, e)
  ) |>
    predict(tibble(X = 1))
  
  TempWrongTree <- lm(
    Y ~ I(X >= 0.5),
    SimData(i, n, e)
  ) |>
    predict(tibble(X = 1))
  
  TempResult <- tibble(
    Pred = c(
      TempAdaptiveTree,
      TempCorrectTree,
      TempWrongTree
    ),
    Method = c(
      "AdaptiveTree",
      "CorrectTree",
      "WrongTree"
    ),
    N = rep(n,3),
    Threshold = rep(TempModel$splits[4],3)
  )
  return(TempResult)
})

Result |>
  ggplot(aes(
    x = N,
    y = Pred
  )) +
  geom_line() +
  facet_wrap(~Method) +
  geom_hline(yintercept = 2) +
  theme_bw()
```

## なぜ？

-   推定するモデルそのものがデータの影響を受けてしまう

    -   予測木において、ハズレ値を少数グループに"隔離"した方が、Fitが改善する

```{r}
Result |>
  ggplot(aes(
    x = N,
    y = Threshold
  )) +
  geom_point() +
  geom_hline(yintercept = 0.9) +
  theme_bw()
```

## 分布: 500サンプル

```{r}
n <- 500
Result <- map_dfr(1:100, function(i) {
  TempModel <- rpart::rpart(Y ~ X,
    SimData(i, n, e),
    control = rpart::rpart.control(
      cp = 0,
      maxdepth = 1,
      minsplit = 4,
      minbucket = 2
    )
  )
  TempAdaptiveTree <- lm_robust(Y ~ 0 + I(X >= TempModel$splits[4]),
                                SimData(i, n, e)) |> 
    tidy() |> 
    mutate(Method = "AdaptiveTree",
           ID = i)
  
  TempCorrectTree <- lm_robust(
    Y ~ 0 + I(X >= 0.9),
    SimData(i, n, e)
  ) |> 
    tidy() |> 
    mutate(Method = "CorrectTree",
           ID = i)
  
  TempWrongTree <- lm_robust(
    Y ~ 0 + I(X >= 0.5),
    SimData(i, n, e)
  ) |> 
    tidy() |> 
    mutate(Method = "WrongTree",
           ID = i)
  
  TempResult <- TempAdaptiveTree |> 
    bind_rows(TempCorrectTree) |> 
    bind_rows(TempWrongTree)
  return(TempResult)
})

Result |> 
  filter(str_detect(term, "TRUE")) |> 
  ggplot(aes(y = ID,
             x = estimate)
         ) +
  geom_point() +
  facet_wrap(~Method) +
  ggside::geom_xsidehistogram() +
  geom_vline(xintercept = 2) +
  theme_bw()
```

## 漸近正規性を用いた信頼区間

```{r}
Result |> 
  filter(str_detect(term, "TRUE")) |> 
  filter(Method == "CorrectTree") |> 
  mutate(Error = if_else(conf.low > 2 |
                          conf.high < 2,
                         "P > 0.05",
                         "P <= 0.05")) |> 
  ggplot(aes(y = ID,
             x = estimate,
             xmin = conf.low,
             xmax = conf.high)
         ) +
  geom_pointrange(aes(color = Error)) +
  facet_wrap(~Method) +
  ggside::geom_xsidehistogram() +
  geom_vline(xintercept = 2) +
  theme_bw()
```

## 漸近正規性を用いた信頼区間

```{r}
Result |> 
  filter(str_detect(term, "TRUE")) |> 
  mutate(Error = if_else(conf.low > 2 |
                          conf.high < 2,
                         "P > 0.05",
                         "P <= 0.05")) |> 
  ggplot(aes(y = ID,
             x = estimate,
             xmin = conf.low,
             xmax = conf.high)
         ) +
  geom_pointrange(aes(color = Error)) +
  facet_wrap(~Method) +
  ggside::geom_xsidehistogram() +
  geom_vline(xintercept = 2) +
  theme_bw() 
```

## 分布: 2000サンプル

```{r}
n <- 2000
Result <- map_dfr(1:100, function(i) {
  TempModel <- rpart::rpart(Y ~ X,
    SimData(i, n, e),
    control = rpart::rpart.control(
      cp = 0,
      maxdepth = 1,
      minsplit = 4,
      minbucket = 2
    )
  )
  TempAdaptiveTree <- lm_robust(Y ~ 0 + I(X >= TempModel$splits[4]),
                                SimData(i, n, e)) |> 
    tidy() |> 
    mutate(Method = "AdaptiveTree",
           ID = i)
  
  TempCorrectTree <- lm_robust(
    Y ~ 0 + I(X >= 0.9),
    SimData(i, n, e)
  ) |> 
    tidy() |> 
    mutate(Method = "CorrectTree",
           ID = i)
  
  TempWrongTree <- lm_robust(
    Y ~ 0 + I(X >= 0.5),
    SimData(i, n, e)
  ) |> 
    tidy() |> 
    mutate(Method = "WrongTree",
           ID = i)
  
  TempResult <- TempAdaptiveTree |> 
    bind_rows(TempCorrectTree) |> 
    bind_rows(TempWrongTree)
  return(TempResult)
})

Result |> 
  filter(str_detect(term, "TRUE")) |> 
  mutate(Error = if_else(conf.low > 2 |
                          conf.high < 2,
                         "P > 0.05",
                         "P <= 0.05")) |> 
  ggplot(aes(y = ID,
             x = estimate,
             xmin = conf.low,
             xmax = conf.high)
         ) +
  geom_pointrange(aes(color = Error)) +
  facet_wrap(~Method) +
  ggside::geom_xsidehistogram() +
  geom_vline(xintercept = 2) +
  theme_bw() 
```

## Target Estimation

- 母分布の部分的特徴を推論

- 典型例: $\beta_D:=E[\tau(x)]$ where $\tau(X)=E[Y|D=1,X]-E[Y|D=0,X]$

    - 因果推論: $X$ 内でDがランダムに決定されているのであれば、平均効果と解釈可能
    
    - 格差研究: $X$ 内平均格差

- セミパラ推定 with 機械学習を用いることでより頑強に推定可能

    - Partialling-outはその一例

## Plug-in 推定

- 最も直接な推定方針は以下

1. $E[Y|D,X]$ を推定: ”予測モデル" $f(d,x)$

2. 各事例に対して予測モデルを適用し、 $\sum_i (f(1,X_i)-f(0,X_i))/N$ を$\beta_D$の推定値とする

- $Y,D,X$ のデータ上での分布(Empirical Distribution)をPlug-inしている

    - OLSもこの一種: $E[Y|D,X]$をOLSで推定

## Pluginの問題点

- $\beta_D$ の推論結果は、$E[Y|D,X]$ の推定精度に決定的に依存する

    - $\beta$の数 $>>$ サンプルサイズ、かつ推定モデルが正しければ、OLSでOK
    
    - ハードルが下がっていない
    
- 誤定式モデルをOLS推定: 一般に$\beta_D$ は一致性すら満たさない

- 機械学習: $\beta_D$ は一致性を満たしうるが、一般に漸近正規性が成り立たない

- どちらも通常の信頼区間が形成できない

## 数値例

```{r}
i <- 4
SimData <- function(i, n) {
  set.seed(i)
  X <- runif(n, -2, 2)
  D <- if_else(X <= 1 & X >= -1,
    sample(0:1,
      n,
      replace = TRUE,
      prob = c(5 / 100, 95 / 100)
    ),
    sample(0:1,
      n,
      replace = TRUE,
      prob = c(95 / 100, 5 / 100)
    )
  )
  Y <- D + X^2 + rnorm(n, 0, 5)
  Temp <- data.table(
    Y = Y,
    X = X,
    D = D,
    TrueY = D + X^2
  )
  return(Temp)
}


Fig1 <- SimData(i, 1000) |>
  mutate(D = factor(D)) |>
  ggplot(aes(
    x = X,
    y = TrueY,
    color = D
  )) +
  geom_smooth() +
  theme_bw() +
  theme(legend.position = "none") +
  ylab("")

Fig2 <- SimData(i, 200) |>
  mutate(D = factor(D)) |>
  ggplot(aes(
    x = X,
    y = Y,
    color = D
  )) +
  geom_point() +
  theme_bw() +
  theme(legend.position = "none") +
  ylab("")

PredLASSO <- gamlr::gamlr(
    x = SimData(i, 200)[, c("X", "D")] |>
      mutate.(X2 = X^2) |>
      as.matrix(),
    y = SimData(i, 200)$Y
  ) |>
    predict(SimData(i, 200)[, c("X", "D")] |>
      mutate.(X2 = X^2) |>
      as.matrix()) |> 
  as.numeric()

PredLASSO1 <- gamlr::gamlr(
    x = SimData(i, 200)[, c("X")] |>
      mutate.(X2 = X^2) |>
      filter.(SimData(i, 200)$D == 1) |> 
      as.matrix(),
    y = SimData(i, 200)$Y[SimData(i, 200)$D == 1]
  ) |>
    predict(SimData(i, 200)[, c("X")] |>
      mutate.(X2 = X^2) |>
      as.matrix()) |> 
  as.numeric()

PredLASSO0 <- gamlr::gamlr(
    x = SimData(i, 200)[, c("X")] |>
      mutate.(X2 = X^2) |>
      filter.(SimData(i, 200)$D == 0) |> 
      as.matrix(),
    y = SimData(i, 200)$Y[SimData(i, 200)$D == 0]
  ) |>
    predict(SimData(i, 200)[, c("X")] |>
      mutate.(X2 = X^2) |>
      as.matrix()) |> 
  as.numeric()

Fig3 <- data.table(
  X = SimData(i, 200)$X,
  D = SimData(i, 200)$D |> factor(),
  Wrong = lm(
    Y ~ D + X,
    SimData(i, 200)
  ) |>
    predict(SimData(i, 200)),
  Collect = lm(
    Y ~ D + X + I(X^2),
    SimData(i, 200)
  ) |>
    predict(SimData(i, 200)),
  LASSO = PredLASSO,
  DoubleLASSO = PredLASSO1 - PredLASSO0
) |> 
  pivot_longer(c(Wrong,Collect,LASSO)) |> 
  ggplot(aes(
    x = X,
    y = value,
    color = D
  )) +
  geom_smooth(se = FALSE) +
  theme_bw() +
  facet_wrap(~name) +
  ylab("")

Fig1 + Fig2 + Fig3
```


## 数値例

```{r}
Est <- function(i) {
  TempData <- SimData(i, 200)

  Y <- TempData$Y
  X <- TempData[, c("X", "D")] |>
    mutate.(X2 = X^2) |>
    as.matrix()
  TempResult <- data.table(
    Est = c(
      coef(lm(Y ~ TempData$D + TempData$X))[2],
      coef(lm(Y ~ TempData$D + TempData$X + I(TempData$X^2)))[2],
      coef(gamlr::gamlr(x = X, y = Y))[3]
    ),
    Method = c(
      "WrongModel",
      "CorrectModel",
      "LASSO"
    ),
    i = i
  )
  return(TempResult)
}

map_dfr.(1:100, Est) |>
  ggplot(aes(
    y = i,
    x = Est
  )) +
  geom_point() +
  ggside::geom_xsidehistogram() +
  geom_vline(xintercept = 1) +
  facet_wrap(~Method) +
  theme_bw()
```


## Research PipeLine

@gruber2022

```{r}
grViz("digraph dot {
      graph [rankdir = UB,color = crimson]
      edge [color=black]
      node [shape = rectangle, color = darkslategray]
      A [label = 'Research Question']
      B [label = '母集団の特徴 (Estimand)']
      B1 [label = 'Descriptive Modelling']
      C [label = 'Score関数の設定']
      D [label = 'Estimandの推定']
      D1 [label = 'Nuisance関数の推定']
      M [label = '機械学習']
      OLS [label = '「伝統的」推定']
      CI [label = '因果推論']
      LR [label = 'セミパラ']
      A -> B
      B -> B1
      B1 -> C
      B -> C
      C -> D
      C -> D1 -> D
      M -> D1
      OLS -> D
      CI -> B
      LR -> C
      {rank = same;D1;D;M}
      {rank = same;CI;B}
      {rank = same;LR;C}
      }")
```

## 例: Research PipeLine

```{r}
grViz("digraph dot {
      graph [rankdir = UB,color = crimson]
      edge [color=black]
      node [shape = rectangle, color = darkslategray]
      A [label = 'Research Question: Dの因果効果']
      B [label = 'F(Y|D=1,X)-F(Y|D=0,X)']
      B1 [label = 'Estimand = E[E[Y|1,X]-E[Y|0,X]]']
      C [label = 'Score: (D-E[D|X])(D-E[D|X]-b(Y-E[Y|D]))']
      D [label = 'Y-E[Y|X]をD-E[D|X]で回帰']
      D1 [label = 'E[D|X],E[Y|X]を推定']
      M [label = 'Stackingなど']
      OLS [label = 'OLS']
      A -> B [label = '条件付きランダム']
      B -> B1 [label = '周辺化平均差']
      B1 -> C [label = 'Partial Linear Model']
      C -> D
      C -> D1 -> D
      M -> D1
      OLS -> D
      {rank = same;D1;D;M}
      }")
```

## まとめ

- 限定的なデータから複雑な社会を推論するために工夫を重ねる

- 母集団を用いた論点整理: Estimandは母集団上で定義 (データが変わっても、不変)
    
- Principle (原理原則)のある選択: 第３者に説明・再現可能 [@urminsky]

    - 機械学習 + 事前知識 (例 @vanderweele2019)

- 教師付き学習 $=$ 条件つき平均値をBlackBoxな関数として学習するための道具

    - 解釈可能なモデリングを行うためのツールとしては使わない

## Reference
