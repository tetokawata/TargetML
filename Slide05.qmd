---
title: "LinearModel"
subtitle: "機械学習の経済学への応用"
author: "川田恵介"
affiliation: "keisukekawata@iss.u-tokyo.ac.jp"
format:
  revealjs:
    incremental: true
    self-contained-math: true
execute: 
  message: false
  warning: false
  echo: false
  eval: true
bibliography: references.bib
---

# Non-Penalized Linear Model

```{r SetUp}
pacman::p_load(tidyverse,
               DiagrammeR,
               patchwork,
               mlr3verse,
               glmnet,
               SuperLearner,
               recipes,
               rpart)

set.seed(111)


lgr::get_logger("mlr3")$set_threshold("fatal")
lgr::get_logger("bbotk")$set_threshold("fatal")

TestData <- tibble(X = seq(-2,2,0.01),
                   D = 1) |> 
  bind_rows(tibble(X = seq(-2,2,0.01),
                   D = 0)) |> 
  mutate(AverageY = D + 2*if_else(X >= 0.5,1,0) + X^2,
         Y = AverageY + rnorm(2*(4/0.01 + 1),0,1))

Sample <- sample(1:nrow(TestData), 
                 40)
```

```{r RealDataExample}
Task <- as_task_regr(read_csv("ExampleData/Example.csv"), 
                     "Price",
                     id = "No PreProcess")

TaskComplex <- recipe(Price ~ .,
                      read_csv("ExampleData/Example.csv")) |> 
  step_interact(~all_numeric_predictors():all_numeric_predictors()) |> 
  step_poly(Size,TradeQ,BuildYear,Distance) |> 
  step_lincomb(all_predictors()) |> 
  step_zv(all_predictors()) |> 
  prep() |> 
  bake(new_data = NULL) |> 
  as_task_regr("Price",
               id = "Duflo's Suggestion")

ElasticNet <- AutoTuner$new(
  learner = lrn("regr.glmnet"),
  resampling = rsmp("cv",folds = 2),
  search_space = ps(
    lambda = p_dbl(lower = 0, upper = 5),
    alpha = p_dbl(lower = 0, upper = 1)
    ),
  terminator = trm("evals",
               n_evals = 100),
  tuner = tnr("random_search")
)

ElasticNet$id <- "ElasticNet"

LASSO <- AutoTuner$new(
  learner = lrn("regr.glmnet"),
  resampling = rsmp("cv",folds = 2),
  search_space = ps(
    lambda = p_dbl(lower = 0, upper = 1)
    ),
  terminator = trm("evals",
               n_evals = 100),
  tuner = tnr("random_search")
)

LASSO$id <- "LASSO"

Design <- benchmark_grid(
  tasks = list(Task,TaskComplex),
  learners = list(
    lrn("regr.lm",
        id = "OLS"),
    lrn("regr.ranger",
        id = "RandomForest"),
    ElasticNet,
    LASSO
  ),
  resamplings = rsmp("cv",folds = 2))

future::plan("multisession")

BenchMark <- benchmark(Design)
```

-   スムーズな予想木と並ぶ人気モデル

    -   NonPenalized Empirical Risk Minimization (OLSなど): 漸近性質が"ほぼほぼ"解明されている

    -   Penalized Empirical Risk Minimization (LASSOなどなど): そもそもの$X$が多いデータにおいて現実的な選択肢

## Linear Model

-   "線形"モデルを事前に設定

$$f(X)=\beta_0 + \beta_1X_1 +..+\beta_LX_L$$

-   非線形モデルも設定可能

$$f(X)=\beta_0 + \beta_1 X + .. + \beta_L X^L$$

## OLS

-   Empirical Risk Minimizationとして、線形モデルを推定

$$\min\sum_{i} (Y_i-f(X_i))^2$$

-   解釈:: 母集団におけるRisk(MSE)を最小化する線形近似 $f_P$ の"優れた"推定量

$$\min_{\beta_0,..,\beta_L}E[(\mu_Y(X)-f_P(X))^2]$$

## 例: Approximation Error

-   $f(X,D)=\beta_0 + \beta_DD + \beta_1X$

```{r}
TempTestData <- tibble(X = seq(0,2,0.01),
                   D = 1) |> 
  bind_rows(tibble(X = seq(0,2,0.01),
                   D = 0)) |> 
  mutate(AverageY = D + X + 0.2*X^2,
         Y = AverageY + rnorm(2*(2/0.01 + 1),0,1))

Pred <- lm(AverageY ~ D + X,
           TempTestData) |> 
  predict(TempTestData)

PredTree <- rpart(AverageY ~ D + X,
                  TempTestData,
                  control = rpart.control(cp = 0,
                                          maxdepth = 10)) |> 
  predict(TempTestData)

TempTestData |> 
  mutate(D = factor(D),
         BLP = Pred) |> 
  ggplot(aes(x = X,
             y = AverageY,
             color = D,
             linetype = "Population")
         ) +
  geom_line() +
  geom_line(aes(y = Pred,
            linetype = "BLP")) +
  geom_line(aes(y = PredTree,
            linetype = "Tree (Depth 10)")) +
  theme_bw()
```

## 例: Approximation Error

-   $f(X,D)=\beta_0 + \beta_DD + \beta_1X$

```{r}
Pred <- lm(AverageY ~ D + X,
           TestData) |> 
  predict(TestData)

PredTree <- rpart(AverageY ~ D + X,
                  TestData,
                  control = rpart.control(cp = 0,
                                          maxdepth = 20)) |> 
  predict(TestData)

TestData |> 
  mutate(D = factor(D),
         BLP = Pred) |> 
  ggplot(aes(x = X,
             y = AverageY,
             color = D,
             linetype = "Population")
         ) +
  geom_line() +
  geom_line(aes(y = Pred,
            linetype = "BLP")) +
  theme_bw()
```

## モデルの複雑化

- 複雑なモデルは容易に想定可能

- 2次モデル

$$f(X,D)=\beta_0 + \beta_DD + \beta_1X + \beta_2X^2$$

- "線形モデル" $=\beta_2 = 0$ という特殊ケース

## 例 Approximation Error

-   $f(X,D)=\beta_0 + \beta_DD + \beta_1X + \beta_2X^2$

```{r}
Pred <- lm(AverageY ~ D + poly(X,2),
           TestData) |> 
  predict(TestData)

TestData |> 
  mutate(D = factor(D),
         BLP = Pred) |> 
  ggplot(aes(x = X,
             y = AverageY,
             color = D,
             linetype = "Population")
         ) +
  geom_line() +
  geom_line(aes(y = Pred,
            linetype = "BLP")) +
  theme_bw()
```

## 例 Approximation Error

-   $f(X,D)=\beta_0 + \beta_DD + \beta_1X + .. + \beta_{25}X^{25}$

```{r}
Pred <- lm(AverageY ~ D + poly(X,25),
           TestData) |> 
  predict(TestData)

TestData |> 
  mutate(D = factor(D),
         BLP = Pred) |> 
  ggplot(aes(x = X,
             y = AverageY,
             color = D,
             linetype = "Population")
         ) +
  geom_line() +
  geom_line(aes(y = Pred,
            linetype = "BLP")) +
  theme_bw()
```

## 復習

-   予測誤差の分解

$$Y-f(X)=\underbrace{Y - \mu_Y(X)}_{IrreducibleError}$$

$$+\underbrace{\mu_Y(X)-f_{\infty}(X)}_{ApproximationError} + \underbrace{f_{\infty}(X) - f(X)}_{EstimationError}$$

## 例 Estimation Error

-   $f(X,D)=\beta_0 + \beta_DD + \beta_1X$

```{r}
Pred <- lm(AverageY ~ D + X,
           TestData) |> 
  predict(TestData)

EstPred <- lm(Y ~ D + X,
           TestData[Sample,]) |> 
  predict(TestData)

Fig1 <- TestData |> 
  mutate(D = factor(D),
         BLP = Pred) |> 
  ggplot(aes(x = X,
             y = AverageY,
             color = D,
             linetype = "Population")
         ) +
  geom_line() +
  geom_line(aes(y = Pred,
            linetype = "BLP")) +
  theme_bw() +
  ylim(0,7) +
  theme(legend.position = "none") +
  ylab("") +
  xlab("Population")

Fig2 <- TestData[Sample,] |> 
  mutate(D = factor(D)) |> 
  ggplot(aes(x = X,
             y = Y,
             color = D)) +
  geom_point() +
  theme_bw() +
  ylim(0,7) +
  theme(legend.position = "none") +
  ylab("") +
  xlab("Data")

Fig3 <- TestData |> 
  mutate(D = factor(D),
         BLP = EstPred) |> 
  ggplot(aes(x = X,
             y = EstPred,
             color = D,
             linetype = "BLP")
         ) +
  geom_line() +
  theme_bw() +
  ylim(0,7) +
  ylab("") +
  xlab("Estimated Model")


Fig1 + Fig2 + Fig3
```

## 例 Estimation Error

-   $f(X,D)=\beta_0 + \beta_DD + \beta_1X + .. + \beta_{25}X^{25}$

```{r}
Pred <- lm(AverageY ~ D + poly(X,15),
           TestData) |> 
  predict(TestData)

EstPred <- lm(Y ~ D + poly(X,15),
           TestData[Sample,]) |> 
  predict(TestData)

Fig1 <- TestData |> 
  mutate(D = factor(D),
         BLP = Pred) |> 
  ggplot(aes(x = X,
             y = AverageY,
             color = D,
             linetype = "Population")
         ) +
  geom_line() +
  geom_line(aes(y = Pred,
            linetype = "BLP")) +
  theme_bw() +
  ylim(-7,7) +
  theme(legend.position = "none") +
  ylab("") +
  xlab("Population")

Fig2 <- TestData[Sample,] |> 
  mutate(D = factor(D)) |> 
  ggplot(aes(x = X,
             y = Y,
             color = D)) +
  geom_point() +
  theme_bw() +
  ylim(-7,7) +
  theme(legend.position = "none") +
  ylab("") +
  xlab("Data")

Fig3 <- TestData |> 
  mutate(D = factor(D),
         BLP = EstPred) |> 
  ggplot(aes(x = X,
             y = EstPred,
             color = D,
             linetype = "BLP")
         ) +
  geom_line() +
  theme_bw() +
  ylim(-7,7) +
  ylab("") +
  xlab("Estimated Model")


Fig1 + Fig2 + Fig3
```

## まとめ

-   "適度"な複雑性をもつモデルを"事前"設定できれば、OLSや最尤法、ベイズ推定は極めて実用的

    -   社会科学では (おそらくBioMedical Scienceでも) 困難

-   複雑すぎるとEstimationError、単純すぎるとApproimationErrorが深刻

    -   古典的な予測モデルは、単純すぎる場合が多いとも [@breiman2001]

# Penalized Linear Model

-   複雑すぎる線形モデルからスタート

-   複雑性へのペナルティーをつけて推定

## LASSO

-   以下のPenalized Empirical Minimizationの解

$$\min\sum_{i} (Y_i-f(X_i))^2 + \lambda [|\beta_1|+..+|\beta_L|]$$

-   $\lambda$ はCrossValidationで決定

    -   代替: AICで決定 (gamlr パッケージ)

## 解釈: 条件付き最適化

-   以下と同値

$$\min\sum_{i} (Y_i-f(X_i))^2\ s.t.\ |\beta_1|+..+|\beta_L| \le A$$

-   $A\rightarrow 0 (\lambda\rightarrow \infty)$ ならば, $f(X)\rightarrow$ サンプル平均

-   $A\rightarrow \infty (\lambda\rightarrow 0)$ ならば, $f(X)\rightarrow$ OLS

-   一般に、サンプル平均とOLSの間

## 例

```{r}
X <- model.matrix(~ 0 + poly(X,15) + D,
                  TestData[Sample,])

TestX <- model.matrix(~ 0 + poly(X,15) + D,
                  TestData)

Y <- TestData[Sample,]$Y

Fit <-cv.glmnet(x = X,
                y = Y)

EstPredLASSO <- predict(Fit, newx = TestX, type = "response", s = "lambda.min")[,1]

Fig1 <- TestData |> 　
  mutate(D = factor(D),
         BLP = Pred) |> 
  ggplot(aes(x = X,
             y = AverageY,
             color = D,
             linetype = "Population")
         ) +
  geom_line() +
  geom_line(aes(y = Pred,
            linetype = "BLP")) +
  theme_bw() +
  ylim(-7,7) +
  theme(legend.position = "none") +
  ylab("") +
  xlab("Population")

Fig2 <- TestData[Sample,] |> 
  mutate(D = factor(D)) |> 
  ggplot(aes(x = X,
             y = Y,
             color = D)) +
  geom_point() +
  theme_bw() +
  ylim(-7,7) +
  theme(legend.position = "none") +
  ylab("") +
  xlab("Data")

Fig3 <- TestData |> 
  mutate(D = factor(D),
         BLP = EstPred) |> 
  ggplot(aes(x = X,
             y = EstPred,
             color = D,
             linetype = "BLP")
         ) +
  geom_line() +
  geom_line(aes(y = EstPredLASSO,
                size = "LASSO")) +
  theme_bw() +
  ylim(-7,7) +
  ylab("") +
  xlab("Estimated Model")


Fig1 + Fig2 + Fig3
```

## 係数値

```{r}
coef(Fit)
```

## Sparcity

-   OLS: $X$ の数 \> サンプルサイズとなると推定不可能

-   Penalized Linear modelやTree系は、可能

    -   LASSOやTree系は、係数値の一部を厳密に "0" として推定

    -   ノイズ的 $X$ が大量に含まれていることが予想される場合、特に重要

-   RandomForestなどと比べても、 $X$ の数が多い時はLASSOに比較優位

    -   目安: サンプルサイズ/4 \> Xの数 [@taddy2019]


## 発展: ElasticNet

-   $\alpha$ , $\lambda$ をCrossValidationで決定

$$\min\sum_{i} (Y_i-f(X_i))^2$$

$$+ \underbrace{\lambda\alpha [|\beta_1|+..+|\beta_L|]}_{L1ペナルティー (LASSO)}$$

$$+ \underbrace{\lambda (1-\alpha) [\beta_1^2+..+\beta_L^2]}_{L2ペナルティー (Ridge)}$$

## Practice

-   Tree系とは異なり、完全なNonparametricモデルの推定ではない

    -   十分に複雑なモデル(OverParametricモデル)からスタートしたい

-   [Duflo's suggestion](https://github.com/demirermert/MLInference/blob/master/NBER_SI_DEV_master_lecture.pdf)

    -   全ての $X$ を投入

    -   連続変数については二乗項、Optionとして交差項

    -   欠損値は欠損ダミーを作成した後に、0を補完

    -   Variationがない変数や完全な多重共線を起こしている変数は除外



## 実例

```{r}
tibble(R2 = BenchMark$aggregate(measure = msr("regr.rsq"))$regr.rsq,
       Learner = BenchMark$aggregate()$learner_id,
       Task = BenchMark$aggregate()$task_id) |> 
  ggplot(aes(x = Learner,
             y = R2)) +
  geom_point() +
  facet_wrap(~Task) +
  theme_bw()
```

## まとめ

- 母平均の複雑すぎるパラメトリックモデルからスタートし、ペナルティー項を用いて単純化

- 決めうちの深い予測木 $+$ Pruningと同じ戦略

## Reference
