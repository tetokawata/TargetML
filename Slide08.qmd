---
title: "Estimate Partial Linear Model"
subtitle: "機械学習の経済学への応用"
author: "川田恵介"
format:
  revealjs:
    incremental: true
    self-contained-math: true
    slide-number: true
execute: 
  message: false
  warning: false
  echo: false
  eval: true
bibliography: references.bib
---

```{r}
pacman::p_load(DiagrammeR,
               tidyverse,
               tidytable,
               recipes,
               mlr3verse,
               patchwork,
               magrittr,
               gamlr,
               estimatr)

SimData <- function(i, n) {
  set.seed(i)
  X <- runif(n, -2, 2)
  Z <- matrix(runif(n*190,-2,2),n,190)
  D <- if_else(X >= 1 | X <= -1,
    sample(0:1, n, replace = TRUE, prob = c(0.9, 0.1)),
    sample(0:1, n, replace = TRUE, prob = c(0.5, 0.5))
  )
  Y <- D + X^2 + rnorm(n, 0, 5)
  HatD <- if_else(X >= 1 | X <= -1, 0.1, 0.5)
  HatY <- X^2 + HatD
  Temp <- as_tibble(Z) |> 
    mutate(D, X, Y,HatD, HatY)
  return(Temp)
}
```

## 問題設定

-   $E[Y|D,X]$ の**特定の**特徴 (Estimand) を推定

-   母集団において、以下を**仮定**

$$E[Y|D,X]=\underbrace{\beta_D}_{Interest} D + \underbrace{f(X)}_{Unkwnon}$$

-   Partial Linear Model [@robinson1988]

-   OLS: $f(X)=\beta_0 + \beta_1X_1+..+\beta_LX_L$ の一般化

-   定式化に誤りがあっても解釈可能(後日)

## 推定アルゴリズム: Partialling-out [@robinson1988]

1.  $E[Y|X] , E[D|X]$ を推定 $\rightarrow f_Y(X),f_D(X)$

-   機械学習 + 交差推定 [@chernozhukov2018]

2.  $Y-f_Y(X)$ を $D - f_D(X)$ で回帰（定数項は含めない）

3.  $f_Y(X),f_D(X)$ の推定誤差を**無視**して、信頼区間をRobustStandardErrorを用いて推定

-   機械学習をNuisance functionを推定するツールとして使用

## 直感

$$Y = \beta_D D + f(X) + \underbrace{u}_{Y-E[Y|D,X]\ (Mean\ Zero)}$$

$$E[Y|X] = \beta_D E[D|X] + f(X)$$

-   両辺を引くと

$$Y-E[Y|X]=\beta_D\times\bigr(D-E[D|X]\bigr)+u$$

## OLSとの関係性: FWL定理

-   FWL定理より、OLS推定は以下のアルゴリズムで書き下せる

1.  $E[Y|X] , E[D|X]$ を推定 $\rightarrow f_Y(X),f_D(X)$

-   *同じOLS + 非交差推定*

2.  $Y-f_Y(X)$ を $D - f_D(X)$ で回帰（定数項は含めない）

3.  $f_Y(X),f_D(X)$ の推定誤差を**無視**して、信頼区間をRobustStandardErrorを用いて推定

## 論点

-   なぜOLSではダメなのか？

    -   過剰適合 あるいは 誤定式化が生じるため

-   なぜ機械学習を応用した他のアルゴリズムではダメなのか？

    -   収束（漸近）性質が悪い

# OLSの問題点

## KeyConcept: Misspecificaition

-   推定モデルの定式化 $=$ 推定されうる関数の**集合**

-   Misspecification: $\beta\in R_{L}$ をどう選んでも、 $f_Y(X)\neq E[Y|X]$

    -   $\beta$ を増やせば、Misspecificationは避けられるが、、、、

## ざっくり性質

-   "正しいモデル": Misspecificationがない + $\beta$ の数 $<<$ サンプルサイズ

    -   真のパラメータへの信頼区間を形成可能

-   "間違ったモデル": Misspefication

    -   線形近似モデルへの信頼区間形成可能 $\neq$ 真のパラメータ

-   過剰なパラメータモデル

    -   過剰適合問題

    -   推定不可能、精度の大幅な悪化、信頼区間爆発

## 信頼区間

-   一般に推定値と真の値は、**一致し得ない**

    -   無限に大きいサンプルサイズが必要

    -   母集団への含意が不明瞭

-   代替的に信頼区間を用いて議論

    -   **高い確率**で真の値を含んだ区間を得られるため

## 復習: Repeated Sampling Framework

```{r VisRepeatedSampling}
grViz("digraph dot {
      graph [rankdir = UB,color = crimson]
      edge [color=black]
      node [shape = rectangle, color = darkslategray]
      A0 [label = 'Not fix everything!!!']
      A1 [label = 'Fix Population and Estimand (defined in Population)']
      A2 [label = 'Fix Sampling Method and Estimation Algorithm']
      A3 [label = 'Fix distributions of population and estimators (unknown)']
      B1 [label = 'Data']
      B2 [label = 'Data']
      B3 [label = 'Data']
      C1 [label = 'Estimators']
      C2 [label = 'Estimators']
      C3 [label = 'Estimators']
      D1 [label= 'Researcher']
      A0 -> A1 -> A2 -> A3
      A3 -> B1,B2,B3
      B1 -> C1
      B2 -> C2
      B3 -> C3 [label = '(Sampling Uncertainly)']
      C1 -> D1
      C2 -> D1 [linetype = dash]
      C3 -> D1 [label = '(unkwnon)']
      {rank = same; B1;B2;B3}
      {rank = same; C1;C2;C3}
      }")
```

## KeyConcepts

-   $\beta_0$ : 母集団におけるパラメータ(Fix but unknown)

-   $\beta_N$ : サンプルサイズ $N$ のデータから計算された推定値 (Random with unknown distribution)

## Key Property

-   大標本

    -   Consistency : $\lim_{N\rightarrow \infty}\bigr(\beta_0 - \beta_{N}\bigr) \rightarrow 0$

        -   経済学では主張の根拠にしずらい

    -   Asymptotic Normality : $\lim_{N\rightarrow \infty} \sqrt{N}\bigr(\beta_0 - \beta_N\bigr)\rightarrow N(0,\sigma^2)$

        -   信頼区間の近似計算の基盤

-   有限標本

    -   Unbiasedness: $E[\beta_N]=\beta$

## OLS + 正しいモデル

-   ランダムサンプルデータであれば、 Asymptotic Normalityを満たす

    -   Consistency, Unbiasednessも

-   独立して研究を行う大量の研究者をイメージ

    -   結論は全員異なる

    -   サンプルが大きくなるにつれて、真の値に近い推定値を得る

    -   **多くの**研究者が真の値を含む信頼区間を得る

## Numerical Example

- $Y = D + X^2 + Normal(0,5)$

- $E[D|X]= 0.5 - 0.4\times I\bigr(X>=1 | X <= -1\bigr)$

- $Data =\{D,X,Z_1,..,Z_{190}\}$

    - $X,Z_1,..,Z_{190}\sim$ Uniform(-2,2)

```{r}
SimCorrect <- function(i,n){
  TempData <- SimData(i,n)
  TempFit <- lm_robust(Y ~ 0 + D + I(X^2),
                       TempData)
  TempResult <- tibble(ID = i,
                       N = n,
                       Est = TempFit$coefficients[2],
                       ConfLow = TempFit$conf.low[2],
                       ConfHigh = TempFit$conf.high[2])
  return(TempResult)
}

SimWrong <- function(i,n){
  TempData <- SimData(i,n)
  TempFit <- lm_robust(Y ~ D + X,
                       TempData)
  TempResult <- tibble(ID = i,
                       N = n,
                       Est = TempFit$coefficients[2],
                       ConfLow = TempFit$conf.low[2],
                       ConfHigh = TempFit$conf.high[2])
  return(TempResult)
}

SimOver <- function(i,n){
  TempData <- SimData(i,n)
  TempFit <- lm_robust(Y ~ D + X + I(X^2) + .,
                       TempData |> 
                         select(Y,
                                D,
                                X,
                                starts_with("V")))
  TempResult <- tibble(ID = i,
                       N = n,
                       Est = TempFit$coefficients[2],
                       ConfLow = TempFit$conf.low[2],
                       ConfHigh = TempFit$conf.high[2])
  return(TempResult)
}

SimDoubleLASSO <- function(i,n){
  TempData <- SimData(i,n)
  X <- model.matrix(~ 0 + X + I(X^2) + I(if_else(X>=1,1,0)) + I(if_else(X>= -1,1,0)) + .,
                    TempData|> 
                         select(X,
                                starts_with("V")))
  Fit <- doubleML(x = X,
                  y = TempData$Y,
                  d = TempData$D) |> 
    summary()
  TempResult <- tibble(ID = i,
                       N = n,
                       Est = Fit$coefficients[1],
                       ConfLow = Fit$coefficients[1] - 1.96*Fit$coefficients[2],
                       ConfHigh = Fit$coefficients[1] + 1.96*Fit$coefficients[2])
  return(TempResult)
}

SimSingleLASSO <- function(i,n){
  TempData <- SimData(i,n)
  X <- model.matrix(~ 0 + D + X + I(X^2) + .,
                    TempData|> 
                         select(X,
                                starts_with("V"),
                                D))
  Fit <- gamlr(x = X[TempData$D == 0,],
               y = TempData$Y[TempData$D == 0]) |> 
    predict(X)
  OhtY <- TempData$Y - Fit |> as.numeric()
  TempFit <- lm_robust(OhtY ~ 0 + TempData$D)
  TempResult <- tibble(ID = i,
                       N = n,
                       Est =   TempFit$coefficients,
                       ConfLow = TempFit$conf.low,
                       ConfHigh = TempFit$conf.high)
  return(TempResult)
}


SimLASSO <- function(i,n){
  TempData <- SimData(i,n)
  X <- model.matrix(~ 0 + D + X + I(X^2) + .,
                    TempData|> 
                         select(X,
                                starts_with("V"),
                                D))
  Fit <- gamlr(x = X,
               y = TempData$Y) |> 
    coef()
  TempResult <- tibble(ID = i,
                       N = n,
                       Est =   Fit[2],
                       ConfLow = as.numeric("NA"),
                       ConfHigh = as.numeric("NA"))
  return(TempResult)
}
```

## PointEstimation

- $E[Y|D,X]=\beta_DD + \beta_{X^2}X^2$

```{r}
CorrectOLS <- map_dfr(1:100,function(i){SimCorrect(i,5000)}) |> 
  bind_rows(map_dfr(1:100,function(i){SimCorrect(i,200)}))

CorrectOLS |> 
  ggplot(aes(y = ID,
             x = Est,
             xmin = ConfLow,
             xmax = ConfHigh)
         ) +
  geom_point() +
  ggside::geom_xsidehistogram() +
  geom_vline(xintercept = 1) +
  theme_bw() +
  facet_wrap(~N)+
  xlim(-4,5)
```

## ConfidenceInterval

```{r}
CorrectOLS |> 
  mutate(Error = if_else(ConfLow > 1 | ConfHigh < 1,
                         "Type-I Error",
                         "OK")
         ) |> 
  ggplot(aes(y = ID,
             x = Est,
             xmin = ConfLow,
             xmax = ConfHigh)
         ) +
  ggside::geom_xsidehistogram() +
  geom_pointrange(aes(color = Error)) +
  geom_vline(xintercept = 1) +
  theme_bw() +
  facet_wrap(~N) +
  xlim(-4,5)
```

## Wrong OLS

- $E[Y|D,X]=\beta_0 + \beta_DD+\beta_1X$

```{r}
WrongOLS <- map_dfr(1:100,function(i){SimWrong(i,5000)}) |> 
  bind_rows(map_dfr(1:100,function(i){SimWrong(i,200)}))

WrongOLS |> 
  mutate(Error = if_else(ConfLow > 1 | ConfHigh < 1,
                         "Type-I Error",
                         "OK")
         ) |> 
 ggplot(aes(y = ID,
             x = Est,
             xmin = ConfLow,
             xmax = ConfHigh)
         ) +
  ggside::geom_xsidehistogram() +
  geom_pointrange(aes(color = Error)) +
  geom_vline(xintercept = 1) +
  theme_bw() +
  facet_wrap(~N) +
  xlim(-4,5)
```

## OverParametrization

- $E[Y|D,X,Z_1,..,Z_{190}]=\beta_D D+ \beta_XX+\beta_{X^2}X^2+\beta_{Z1}Z_1+..+\beta_{Z_{190}}Z_{190}$

```{r}
OverOLS <- map_dfr(1:100,function(i){SimOver(i,5000)}) |> 
  bind_rows(map_dfr(1:100,function(i){SimOver(i,200)}))

OverOLS |> 
  write_csv("TempResult/OverOLS.csv")

read_csv("TempResult/OverOLS.csv") |> 
  mutate(Error = if_else(ConfLow > 1 | ConfHigh < 1,
                         "Type-I Error",
                         "OK")
         ) |> 
  ggplot(aes(y = ID,
             x = Est,
             xmin = ConfLow,
             xmax = ConfHigh)
         ) +
  ggside::geom_xsidehistogram() +
  geom_pointrange(aes(color = Error)) +
  geom_vline(xintercept = 1) +
  theme_bw() +
  facet_wrap(~N) +
  xlim(-4,5)
```

## OverParametrization

```{r}
read_csv("TempResult/OverOLS.csv") |> 
  mutate(Error = if_else(ConfLow > 1 | ConfHigh < 1,
                         "Type-I Error",
                         "OK")
         ) |> 
  ggplot(aes(y = ID,
             x = Est,
             xmin = ConfLow,
             xmax = ConfHigh)
         ) +
  ggside::geom_xsidehistogram() +
  geom_pointrange(aes(color = Error)) +
  geom_vline(xintercept = 1) +
  theme_bw() +
  facet_wrap(~N)
```


## OLSまとめ

-   OLS + 少数のパラメタ without miss-speficaition: 素晴らしいパフォーマンス

    -   非現実的？

-   OLS + Miss-speficaition

    -   Invalid Confidence Interval, Non-Consistent

-   OLS + 大量のパラメータ

    - 過剰適合、標準誤差の爆発、信頼区間が広すぎる\|計算できない

## 付録: RCT

- シンプルな線形モデルを用いた因果推論は、"TopJournal"でも散見される

- 重要な例外: $D$がランダムに決定

    - $X$ を導入しても漸近性質は悪化しない [@lin2013]
    
    - "Top Journal"における因果推論の多くは、RCT | 綺麗な自然実験を用いたものが多い

- 機械学習の応用にも有益

## 付録: Saturated Model

- $f(X)=$ ありうる全ての$X$の組み合わせについてのダミー変数を導入し、OLS推定 [@angrist2009]

    - Saturated Model

- Partial Linear Modelの範囲内では誤定式化は起きない

    - より一般的なケースでも、解釈可能 (後日)
    
- 過剰適合は大丈夫???

# 機械学習の応用

-   基本戦略「過剰に複雑なモデルを適度に単純化する」を踏襲

-   Partialling-outを推奨

    - Nuisance関数の推定に機械学習を使用

## 非推奨: MachineLearning as Modelling

-   $E[Y|D,X]$ を直接推定

-   例: 以下を推定

$$\min\sum_{i}\bigr[Y-\beta_D\times D-\underbrace{\tilde f(X)}_{十分に複雑な線形モデル}\bigr]^2+\lambda\sum_{l}|\beta_l|$$

-   LASSO

## 非推奨: Single MachineLearning

1.  $D\in \{0,1\}$を想定

2.  $E[Y|D=0,X]$ を推定 $\rightarrow f_{Y0}(X)$

3.  $Y-f_{Y0}(X)$ を $D$ で回帰

## LASSO

- $f(X)=\beta_0 + \beta_{X}X+\beta_{X^2}X^2+\beta_{Z_1}Z_1+..+\beta_{190}Z_{190}$

```{r}
LASSO <- map_dfr(1:100,function(i){SimLASSO(i,5000)}) |> 
  bind_rows(map_dfr(1:100,function(i){SimLASSO(i,200)}))

LASSO |> 
  write_csv("TempResult/LASSO.csv")

read_csv("TempResult/LASSO.csv") |> 
  ggplot(aes(y = ID,
             x = Est)
         ) +
  ggside::geom_xsidehistogram() +
  geom_point() +
  geom_vline(xintercept = 1) +
  theme_bw() +
  facet_wrap(~N) +
  xlim(-4,5)
```


## Single LASSO

- $f_{Y0}(X)=\beta_0 + \beta_{X}X+\beta_{X^2}X^2+\beta_{Z_1}Z_1+..+\beta_{190}Z_{190}$

```{r}
SingleLASSO <- map_dfr(1:100,function(i){SimSingleLASSO(i,5000)}) |> 
  bind_rows(map_dfr(1:100,function(i){SimSingleLASSO(i,200)}))

SingleLASSO |> 
  write_csv("TempResult/SingleLASSO.csv")

read_csv("TempResult/SingleLASSO.csv") |> 
  mutate(Error = if_else(ConfLow > 1 | ConfHigh < 1,
                         "Type-I Error",
                         "OK")
         ) |> 
  ggplot(aes(y = ID,
             x = Est,
             xmin = ConfLow,
             xmax = ConfHigh,
             color = Error)
         ) +
  geom_pointrange() +
  geom_vline(xintercept = 1) +
  theme_bw() +
  facet_wrap(~N) +
  xlim(-4,5)
```


## Double LASSO

- $f_{Y}(X)=\beta_0 + \beta_{X}X+\beta_{X^2}X^2 + \beta_{1}I(X\ge-1)+\beta_2I(X\ge-1)+\beta_{Z_1}Z_1+..+\beta_{190}Z_{190}$

- $f_{D}(X)=\beta_0 + \beta_{X}X+\beta_{X^2}X^2+ \beta_{1}I(X\ge-1)+\beta_2I(X\ge-1)+\beta_{Z_1}Z_1+..+\beta_{190}Z_{190}$


```{r}
DoubleLASSO <- map_dfr(1:100,function(i){SimDoubleLASSO(i,5000)}) |> 
  bind_rows(map_dfr(1:100,function(i){SimDoubleLASSO(i,200)}))

DoubleLASSO |> 
  write_csv("TempResult/DoubleLASSO.csv")

read_csv("TempResult/DoubleLASSO.csv") |> 
  mutate(Error = if_else(ConfLow > 1 | ConfHigh < 1,
                         "Type-I Error",
                         "OK")
         ) |> 
  ggplot(aes(y = ID,
             x = Est,
             xmin = ConfLow,
             xmax = ConfHigh)
         ) +
  ggside::geom_xsidehistogram() +
  geom_pointrange(aes(color = Error)) +
  geom_vline(xintercept = 1) +
  theme_bw() +
  facet_wrap(~N) +
  xlim(-4,5)
```

## まとめ: 機械学習

- Population Risk (MSE) を最小化するように設計

    - パラメータの推論が主目的ではない

- 非常に緩やかな仮定の元で、一致性が成り立つアルゴリズムは複数存在

- 一般に（有限標本）バイアスが発生

- 収束が遅い、漸近正規性が成り立たなず、信頼区間計算が難しい

    - Bootstrap法でも同じ
    
## まとめ: 理想的なOLS

$$\beta_{interest}-\beta_{N}=\underbrace{\beta_{interest}-\beta_0}_{Identification}$$

$$+\underbrace{\beta_0 - \beta_{N\rightarrow\infty}}_{=0}+\underbrace{\beta_{N\rightarrow\infty}-E[\beta_{N}]}_{=0}+\underbrace{E[\beta_{N}]-\beta_{N}}_{正規分布で近似}$$

- 一般に $\lim_{N\rightarrow\infty}\sqrt{N}(\beta_N-\beta_0)\rightarrow Normal$

## まとめ: 誤定式化

$$\beta_{interest}-\beta_{N}=\underbrace{\beta_{interest}-\beta_0}_{Identification}$$

$$+\underbrace{\beta_0 - \beta_{N\rightarrow\infty}}_{\neq0}+\underbrace{\beta_{N\rightarrow\infty}-E[\beta_{N}]}_{=0}+\underbrace{E[\beta_{N}]-\beta_{N}}_{正規分布で近似}$$

- 一般に $\lim_{N\rightarrow\infty}\sqrt{N}(\beta_N-\beta_0)\rightarrow\infty$

## まとめ: 複雑すぎるモデル

$$\beta_{interest}-\beta_{N}=\underbrace{\beta_{interest}-\beta_0}_{Identification}$$

$$+\underbrace{\beta_0 - \beta_{N\rightarrow\infty}}_{=0}+\underbrace{\beta_{N\rightarrow\infty}-E[\beta_{N}]}_{=0}+\underbrace{E[\beta_{N}]-\beta_{N}}_{爆発の恐れ}$$

- 一般に $\lim_{N\rightarrow\infty}\sqrt{N}(\beta_N-\beta_0)\rightarrow Normal$ だが、、、

## まとめ: LASSO

$$\beta_{interest}-\beta_{N}=\underbrace{\beta_{interest}-\beta_0}_{Identification}$$

$$+\underbrace{\beta_0 - \beta_{N\rightarrow\infty}}_{"=0"}+\underbrace{\beta_{N\rightarrow\infty}-E[\beta_{N}]}_{\neq 0}+\underbrace{E[\beta_{N}]-\beta_{N}}_{SamplingUncetainly}$$

- 一般に $\lim_{N\rightarrow\infty}\sqrt{N}(\beta_N-\beta_0)\rightarrow\infty$

## まとめ: Single MachineLearning

$$\beta_{interest}-\beta_{N}=\underbrace{\beta_{interest}-\beta_0}_{Identification}$$

$$+\underbrace{\beta_0 - \beta_{N\rightarrow\infty}}_{"=0"}+\underbrace{\beta_{N\rightarrow\infty}-E[\beta_{N}]}_{\neq 0}+\underbrace{E[\beta_{N}]-\beta_{N}}_{SamplingUncetainly}$$

- 一般に $\lim_{N\rightarrow\infty}\sqrt{N}(\beta_N-\beta_0)\rightarrow\infty$

## まとめ: PartiallingOut

$$\beta_{interest}-\beta_{N}=\underbrace{\beta_{interest}-\beta_0}_{Identification}$$

$$+\underbrace{\beta_0 - \beta_{N\rightarrow\infty}}_{"=0"}+\underbrace{\beta_{N\rightarrow\infty}-E[\beta_{N}]}_{"\rightarrow 0"}+\underbrace{E[\beta_{N}]-\beta_{N}}_{正規分布で近似}$$

- 一般に $\lim_{N\rightarrow\infty}\sqrt{N}(\beta_N-\beta_0)\rightarrow Normal$

## Reference
